# Rule Database

TermSat works by using a database of rewrite rules to reduce propositional formulas.  
The TermSAT Rule db is based on PostgreSQL.
Termat contains several utilities that build the rule db.  
Building the db is quite difficult, technically.  
There are 100s of millions of formulas to sort through, so the building process must be fast.  
I've learned that fast also means scalable.  
My first attempts to build a multi-threaded app using sqlite were quite a failure.  
I was forced to come up a different, more scalable and resilient way of building the db.  
The new way is to use a client/server database and a scalable pool of workers that cooperate 
to built the database.  
This document describes the new database and the workers.

## Tables

### Rules Table

This table is a list of all formulas generated by the rule sieve procedure.  
Each non-canonical formula in the db is the left side of a 'rule, 
where the right side of the rule is the corresponding canonical formula.  

#### Columns
@see TermSAT.RuleDatabase.FormulaRecord

Every rule has the following properties...
- Id : auto-generated as records as added
- Text : formula as string
- Length : the length of the formula, not the same as Text.Length
- VarCount : The number of unique variables in the formula, from 1 to VarCount
- TruthValue : The TruthValue of the formula, in hex
- IsCanonical : true/false.  

#### Formula Order
The TermSAT [*formula order*](formulas.md#ordering) is equivalent to selecting formulas ordered by VarCount, Length, & Text.
Like so...dbContext.Rules.OrderBy(_ => _.VarCount).ThenBy(_ => _.Length).ThenBy(_ => _.Text).AsEnumerable();
Or... dbContext.Rules.InFormulaOrder().AsEnumerable();

### Lookup Table

This table defines a prefix tree used to efficiently find rules that can be applied to a given formula.

### Log Table

A simple log, just a timestamp and a message

### Meta Table

A simple meta table, with columns: string Object, string Predicate, string? Value

## The Build Process
The build process uses a client/server database and a scalable pool of workers that cooperate to built the database.  

However, we cant just let workers run wild, the build process *must* proceed in an orderly fashion.  
First, variables are completed sequentially, from .1 to N.
Second, while completing each variable, formulas are completed in formula order, 
that is, from the shortest formulas to the longest, sequentially increasing length.  
All the formulas of a given length may be completed concurrently (because we can know that 
none of these formulas reduce any of the other formulas of the same length).

The state of the build process is saved in the Meta table....
```
BuildProcess, CurrentVarNumber, "1"
BuildProcess, CurrentLength, "3"
```


### Worker Design

#### Concurrency
Workers continuously select (using FOR UPDATE SKIP_LOCKED) formulas to process.  
*FOR UPDATE SKIP_LOCKED* is how workers avoid stepping on each other.  
Therefore, only a single worker will be able to process a formula at a time.

#### Build State
Workers must always respect the current build state, that is, the current 


### The Build Workers

The db build process produces the formulas that populate the rule and index databases.  
The process starts by adding the variable formula .1, and then producing its transitive closure.
Then .2 is added and it closure is produced.  Then .2 and so on.  
Hopefully, when we complete .7 the completion process will terminate.

The build process is composed of a number of steps that produce the rule and index databases.  
The build process moves each formula through a number of states, in the order specified below.  
Also, formulas must be processed in formula order, but not necessarily sequentially, 
so there can be many uncompleted formulas in any one of many possible stages.  
Thus, we can scale up the processing by designing workers to perform one of the steps, 
but for any number of formulas at once.  

The basic steps, in order, are meant implement the Knuth-Bendix Completion algorithm...   
- Extend: Add a new Variable, and generate the closure of the new variable with the formulas previously added to the database.  
	> If a formula is subsumed by a previous non-canonical formula in the Lookup then exclude it from the db.  
	> Formulas are generated in formula order, from length == 3 to max canonical length * 2, 
	> where max canonical length is the length of the longest canonical formula with Build.VariableNumber variables.
- Evaluate: Calculate truth values for all new, non-subsumed formulas added in the Extend.
- Index: After all formulas have been evaluated, add non-canonical formulas to the lookup database for use in the Extend

Repeat the above steps until extending the system produces no new non-canonical formulas.  
That is, until all formulas in the Extend step are subsumed.  

Terminology...   
- A *previous* formula is a formula that comes before some other formula in TermSAT's formula ordering.  


#### Extend 
If no variable formulas exist in the database then adds .1 to the database.  
Otherwise adds a new variable formula to the database where the 
new variable has a number equal to 1 more than the highest numbered variable in the db.

After adding a new variable, workers will continue to process tasks until the variable 
is 'closed'.  
That is, until the workers have produced the transitive closure of the variable.  

When the rule db is *complete* (in the Knuth-Bendix sense) then for all records _.IsReducible == true || _.IsClosed == true.

#### The Monitor
Continuously selects and prints out new Log messages.  Like the tail command.  

#### The Reducer
1st thing, we need to avoid adding formulas that are reducible to the database.  
This is because, the goal is to run the Knuth-Bendix completion method to termination, over formulas with at least 7 variables, maybe more.  
Keeping track of every formula that's generated by the Knuth-Bendix method could mean 3.4e+38 formulas in the database.  
Maybe that's possible, I dunno, but I think it's best to avoid the ass-kicking that attempting to build such a database might get me.  
The vast majority of those formulas will eventually *not* be processed if leave subsumable formulas out of the database.  
But to be able to check a formula before its added to the db we'll need to generate formulas in the formula order, 
not the usual depth-first Knuth-Bendix order.  

So this worker...
...selects formulas where IsReducible == null 
...checks whether the formula is reducible by any formulas in the Lookup db.  
...deletes formulas that are reducible
...updates IsReducible for non-reducible formulas.
Note, new formulas are built from canonical formulas (which are not reducible), 
so only need to check the new formula, not sub-formulas.

Other workers are required to always select formulas where IsReducible == false 

#### The Evaluator
We need to know the truth value of every formula before we can do anything with it.  
The evaluator just calculates truth values, it...   
...selects all records where IsReducible == false && the TruthValue is null.  
...calculates truth values for all those formulas.  
...saves the calculated values to the db.  
Its probably most efficient to save all the calculated values in a single transaction.  

#### The Indexer
Add formulas where IsReducible == false && IsIndexed == null to the Lookup db, then set IsIndexed to true.

#### The Closer
Generates new formulas, of Length == Meta.Where(BuildProcess.CurrentLength), from existing canonical formulas.  
Closing a formula means constructing all possible formulas in the 'closure' of the new formula and all the canonical 
formulas previously added to the database.  
New formulas are not checked to see if they're reducible before they're inserted into the database.  
Note, new formulas that are reducible will eventually be removed by a Reducer.  
A closer...
...selects FormulaRecords.InFormulaOrder().Where(IsReducible == false && TruthValue != null && IsClosed == null).First();
	AND 
...pairs the selected formula with all previous canonical formulas and adds them to the db 
...sets IsClosed to true



## NOTES



### Row-Level Locking
Here's a good article...  
https://www.milanjovanovic.tech/blog/a-clever-way-to-implement-pessimistic-locking-in-ef-core